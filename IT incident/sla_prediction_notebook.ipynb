{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed24ccb",
   "metadata": {},
   "source": [
    "# IT Incident SLA Breach Prediction\n",
    "This notebook:\n",
    "- Reads raw event log `sla_incident_events.csv`\n",
    "- Cleans data, resolves SLA inconsistencies\n",
    "- Uses only the **first 3 events** per incident to build features\n",
    "- Trains a baseline (Logistic Regression) and an advanced model (XGBoost)\n",
    "- Interprets early signals and performs a counterfactual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef93dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENTS_CSV = \"sla_incident_events.csv\"\n",
    "\n",
    "events = pd.read_csv(EVENTS_CSV, parse_dates=['timestamp'])\n",
    "print(\"Loaded events:\", events.shape)\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558dc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Drop exact duplicates\n",
    "events = events.drop_duplicates()\n",
    "\n",
    "# 2) Drop rows missing key values\n",
    "events = events.dropna(subset=['incident_id', 'timestamp'])\n",
    "\n",
    "# 3) Resolve SLA label inconsistencies per incident using majority vote\n",
    "def resolve_sla_majority(g):\n",
    "    vc = g['sla_breached'].value_counts()\n",
    "    if vc.empty:\n",
    "        chosen = 0\n",
    "    elif len(vc) == 1:\n",
    "        chosen = vc.idxmax()\n",
    "    else:\n",
    "        # majority; if tie, choose 1\n",
    "        if vc.iloc[0] == vc.iloc[1]:\n",
    "            chosen = 1\n",
    "        else:\n",
    "            chosen = vc.idxmax()\n",
    "    g['sla_breached'] = chosen\n",
    "    return g\n",
    "\n",
    "events = events.groupby('incident_id', group_keys=False).apply(resolve_sla_majority)\n",
    "\n",
    "# 4) Quick data quality\n",
    "print(\"Unique incidents:\", events['incident_id'].nunique())\n",
    "print(\"Missing values per column:\\n\", events.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac744a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and take first 3 events per incident\n",
    "events_sorted = events.sort_values(['incident_id','timestamp'])\n",
    "first3 = events_sorted.groupby('incident_id', group_keys=False).head(3).copy()\n",
    "\n",
    "# Add event order to make pivoting robust\n",
    "first3['event_order'] = first3.groupby('incident_id').cumcount() + 1\n",
    "\n",
    "# Pivot to wide format (activity1, actor1, ts1, activity2, actor2, ts2, ...)\n",
    "wide = first3.pivot(index='incident_id', columns='event_order',\n",
    "                    values=['event_type','actor','timestamp'])\n",
    "\n",
    "# Flatten multiindex columns\n",
    "wide.columns = [f\"{col[0]}{col[1]}\" for col in wide.columns]\n",
    "wide = wide.reset_index()\n",
    "\n",
    "# Bring SLA target (we resolved inconsistencies above)\n",
    "targets = events.groupby('incident_id', as_index=False)['sla_breached'].max()\n",
    "data = pd.merge(wide, targets, on='incident_id', how='inner')\n",
    "\n",
    "print(\"Wide data (one row per incident):\", data.shape)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59661988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure timestamp columns are datetimes\n",
    "for tcol in ['timestamp1','timestamp2','timestamp3']:\n",
    "    if tcol in data.columns:\n",
    "        data[tcol] = pd.to_datetime(data[tcol], errors='coerce')\n",
    "\n",
    "# Temporal deltas in minutes (use safe calculation)\n",
    "def minutes_diff(a,b):\n",
    "    return ((a - b).dt.total_seconds() / 60.0).fillna(0.0)\n",
    "\n",
    "if 'timestamp1' in data.columns and 'timestamp2' in data.columns:\n",
    "    data['dt_1_2'] = minutes_diff(data['timestamp2'], data['timestamp1'])\n",
    "else:\n",
    "    data['dt_1_2'] = 0.0\n",
    "\n",
    "if 'timestamp2' in data.columns and 'timestamp3' in data.columns:\n",
    "    data['dt_2_3'] = minutes_diff(data['timestamp3'], data['timestamp2'])\n",
    "else:\n",
    "    data['dt_2_3'] = 0.0\n",
    "\n",
    "# dt_1_3 if both exist\n",
    "if 'timestamp1' in data.columns and 'timestamp3' in data.columns:\n",
    "    data['dt_1_3'] = minutes_diff(data['timestamp3'], data['timestamp1'])\n",
    "else:\n",
    "    data['dt_1_3'] = data['dt_1_2'] + data['dt_2_3']\n",
    "\n",
    "# Derived flags from activities and actors\n",
    "for col in ['event_type1','event_type2','event_type3','actor1','actor2','actor3']:\n",
    "    if col not in data.columns:\n",
    "        data[col] = np.nan\n",
    "\n",
    "data['any_escalated_1_3'] = data[['event_type1','event_type2','event_type3']].apply(\n",
    "    lambda row: int(any(isinstance(x,str) and 'escal' in x.lower() for x in row)), axis=1)\n",
    "data['any_assigned_1_3'] = data[['event_type1','event_type2','event_type3']].apply(\n",
    "    lambda row: int(any(isinstance(x,str) and 'assign' in x.lower() for x in row)), axis=1)\n",
    "data['tier2_in_first3'] = data[['actor1','actor2','actor3']].apply(\n",
    "    lambda row: int(any(isinstance(x,str) and 'tier2' in x.lower() for x in row)), axis=1)\n",
    "\n",
    "# Calendar features from first timestamp\n",
    "data['first_ts'] = data['timestamp1']\n",
    "data['hour_of_day'] = data['first_ts'].dt.hour.fillna(0).astype(int)\n",
    "data['day_of_week'] = data['first_ts'].dt.weekday.fillna(0).astype(int)\n",
    "data['is_weekend'] = (data['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Optionally drop the raw timestamp columns to avoid leak\n",
    "drop_cols = [c for c in ['timestamp1','timestamp2','timestamp3','first_ts'] if c in data.columns]\n",
    "data = data.drop(columns=drop_cols)\n",
    "\n",
    "# Fill remaining NaNs for categorials with 'Unknown'\n",
    "cat_cols = [c for c in data.columns if c.startswith('event_type') or c.startswith('actor') or c in ['priority','incident_type','affected_system'] if c in data.columns]\n",
    "for c in cat_cols:\n",
    "    data[c] = data[c].fillna('Unknown')\n",
    "\n",
    "# Fill numeric NaNs\n",
    "for c in ['dt_1_2','dt_2_3','dt_1_3']:\n",
    "    data[c] = data[c].fillna(data[c].median())\n",
    "\n",
    "# Quick check\n",
    "print(\"Feature columns:\", [c for c in data.columns if c not in ['incident_id','sla_breached']][:30])\n",
    "data[['dt_1_2','dt_2_3','dt_1_3','any_escalated_1_3','any_assigned_1_3','tier2_in_first3']].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_CSV = \"sla_incident_features_first3_from_raw.csv\"\n",
    "data.to_csv(FEATURES_CSV, index=False)\n",
    "print(\"Saved features csv:\", FEATURES_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99389ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "y = data['sla_breached'].astype(int)\n",
    "X = data.drop(columns=['incident_id','sla_breached'])\n",
    "\n",
    "# Define feature groups\n",
    "numeric_features = [c for c in X.columns if X[c].dtype in [np.float64, np.int64] and not c.startswith('event_type') and not c.startswith('actor')]\n",
    "categorical_features = [c for c in X.columns if c not in numeric_features]\n",
    "\n",
    "print(\"Numeric features:\", numeric_features)\n",
    "print(\"Categorical features (sample):\", categorical_features[:10])\n",
    "\n",
    "# Train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n",
    "print(\"Train/test sizes:\", X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numeric_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical_features)\n",
    "])\n",
    "\n",
    "baseline = Pipeline([\n",
    "    ('preprocess', preprocess),\n",
    "    ('clf', LogisticRegression(max_iter=500, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "proba_bl = baseline.predict_proba(X_test)[:,1]\n",
    "pred_bl = (proba_bl >= 0.5).astype(int)\n",
    "\n",
    "print(\"Baseline metrics:\")\n",
    "print(\"AUC:\", round(roc_auc_score(y_test, proba_bl), 4))\n",
    "print(classification_report(y_test, pred_bl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc89e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training data for XGBoost\n",
    "X_train_trans = preprocess.fit_transform(X_train)\n",
    "X_test_trans  = preprocess.transform(X_test)\n",
    "\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "xgb_clf.fit(X_train_trans, y_train, eval_set=[(X_test_trans, y_test)], verbose=False)\n",
    "\n",
    "proba_xgb = xgb_clf.predict_proba(X_test_trans)[:,1]\n",
    "pred_xgb = (proba_xgb >= 0.5).astype(int)\n",
    "\n",
    "print(\"XGBoost metrics:\")\n",
    "print(\"AUC:\", round(roc_auc_score(y_test, proba_xgb), 4))\n",
    "print(classification_report(y_test, pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523682f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "fpr_bl, tpr_bl, _ = roc_curve(y_test, proba_bl)\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, proba_xgb)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr_bl, tpr_bl, label=f'LogReg (AUC={roc_auc_score(y_test, proba_bl):.3f})')\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC={roc_auc_score(y_test, proba_xgb):.3f})')\n",
    "plt.plot([0,1],[0,1],'--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(); plt.title(\"ROC Curves\"); plt.show()\n",
    "\n",
    "# Confusion matrix for XGBoost\n",
    "cm = confusion_matrix(y_test, pred_xgb)\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Breached','Breached'], yticklabels=['Not Breached','Breached'])\n",
    "plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.title('Confusion Matrix (XGBoost)'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53399b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature names after preprocessing\n",
    "ohe = preprocess.named_transformers_['cat']\n",
    "ohe_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "feature_names = numeric_features + ohe_names\n",
    "\n",
    "# LR coefficients\n",
    "lr_coef = baseline.named_steps['clf'].coef_[0]\n",
    "lr_imp = sorted(zip(feature_names, lr_coef), key=lambda x: abs(x[1]), reverse=True)[:15]\n",
    "print(\"Top Logistic Regression signals (feature, coef):\")\n",
    "for f,c in lr_imp:\n",
    "    print(f, round(c,4))\n",
    "\n",
    "# XGBoost importances\n",
    "xgb_importances = xgb_clf.feature_importances_\n",
    "imp_df = pd.DataFrame({'feature': feature_names, 'importance': xgb_importances})\n",
    "imp_df = imp_df.sort_values('importance', ascending=False).head(15)\n",
    "print(\"\\nTop XGBoost features:\")\n",
    "print(imp_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9519642",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plain-language takeaways (examples):\")\n",
    "print(\"- Larger dt_1_2 or dt_1_3 (long idle time in early events) tends to increase breach risk.\")\n",
    "print(\"- Early escalation or Tier2 assignment presence are strong signals (direction depends on data).\")\n",
    "print(\"- High priority (Critical/High) raises baseline breach probability.\")\n",
    "print(\"- If 'Investigating' appears without an early assignment, that's often a warning sign.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa641cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one high-risk incident from test set (by XGBoost probability) and show before/after\n",
    "proba_bl_test = baseline.predict_proba(X_test)[:,1]\n",
    "idx_high = int(np.argmax(proba_bl_test))\n",
    "orig = X_test.iloc[[idx_high]].copy()\n",
    "\n",
    "print(\"Original sample (selected cols):\")\n",
    "display(orig[['event_type1','event_type2','event_type3','actor1','actor2','actor3','dt_1_2','dt_2_3']].T)\n",
    "\n",
    "p_before = baseline.predict_proba(orig)[:,1][0]\n",
    "\n",
    "# Counterfactual: assign event2 to 'Assigned' and actor2 to 'Tier2' and shorten dt_1_2\n",
    "cf = orig.copy()\n",
    "if 'event_type2' in cf.columns:\n",
    "    cf.loc[:, 'event_type2'] = 'Assigned'\n",
    "if 'actor2' in cf.columns:\n",
    "    cf.loc[:, 'actor2'] = 'Tier2'\n",
    "if 'dt_1_2' in cf.columns:\n",
    "    cf.loc[:, 'dt_1_2'] = max(1.0, cf.loc[:, 'dt_1_2'] * 0.1)  # shorten to 10% or at least 1 minute\n",
    "\n",
    "p_after = baseline.predict_proba(cf)[:,1][0]\n",
    "\n",
    "print(f\"Predicted risk BEFORE: {p_before:.3f}\")\n",
    "print(f\"Predicted risk AFTER (counterfactual): {p_after:.3f}\")\n",
    "print(f\"Delta: {p_after - p_before:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model:\n",
    "import joblib\n",
    "joblib.dump(preprocess, \"preprocess_pipeline.joblib\")\n",
    "joblib.dump(baseline, \"baseline_logreg_pipeline.joblib\")\n",
    "joblib.dump(xgb_clf, \"xgb_model.joblib\")\n",
    "print(\"Saved models and preprocessors.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
